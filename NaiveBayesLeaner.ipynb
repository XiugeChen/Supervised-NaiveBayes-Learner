{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s): Xiuge Chen 961392\n",
    "###### Python version: python3\n",
    "###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use different test strategy, change the NUM_PARTITION in SYSTEM SETTINGS, 1 represents test on training data, \n",
    "# other represents k-fold cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SYSTEM SETTINGS #####\n",
    "\n",
    "# data folder path\n",
    "FOLDER_PATH = \"./2019S1-proj1-data/\"\n",
    "# file name that contains header information\n",
    "HEADER_FILE = \"headers.txt\"\n",
    "# epsilon smoothing value\n",
    "EPSILON = sys.float_info.epsilon\n",
    "# number of partition used in cross validation, 1 if testing on training data\n",
    "NUM_PARTITION = 1\n",
    "\n",
    "##### END SYSTEM SETTINGS #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DATA PROCESSING ####\n",
    "# opens a data file, and converts it into a usable format dataset with proper header at its first row\n",
    "# INPUT: fileName that contains data\n",
    "# OUTPUT: dataset, a list of instance, where instance is a 2-tuple \n",
    "# (a list of attribute values, and a class label attribute)\n",
    "def preprocess(fileName):\n",
    "    dataSet = []\n",
    "    \n",
    "    # get headers, transform into instance and append it to the first line of dataset\n",
    "    headerArray = getHeader(fileName)\n",
    "    dataSet.append((headerArray[:len(headerArray) - 1], headerArray[len(headerArray) - 1]))\n",
    "    \n",
    "    # get data, transform into instance and append it to dataset\n",
    "    dataFile = open(FOLDER_PATH + fileName, 'r')\n",
    "    for line in dataFile:\n",
    "        line = line.rstrip('\\n')\n",
    "        dataArray = line.split(',')\n",
    "        \n",
    "        dataSet.append((dataArray[:len(dataArray) - 1], dataArray[len(dataArray) - 1]))\n",
    "    \n",
    "    return dataSet\n",
    "\n",
    "# given a fileName, return the header that decripts to this data file\n",
    "# INPUT: file name\n",
    "# OUTPUT: array of elements in header\n",
    "def getHeader(fileName):\n",
    "    headerFile = open(FOLDER_PATH + HEADER_FILE, 'r') \n",
    "    lines = headerFile.readlines()\n",
    "    \n",
    "    index = lines.index(fileName + '\\n')   \n",
    "    header = lines[index + 2]\n",
    "    header = header.rstrip('\\n')\n",
    "    headerArray = header.split(',')\n",
    "    \n",
    "    return headerArray\n",
    "\n",
    "# given a folder that contains all files, get all csv file names\n",
    "# INPUT: folder path\n",
    "# OUTPUT: csv file names\n",
    "def getFileNames(folderPath):\n",
    "    filePaths = []\n",
    "    \n",
    "    for dicName in os.listdir(folderPath):\n",
    "        if dicName.startswith('.') or not dicName.endswith(\".csv\"):\n",
    "            continue\n",
    "        \n",
    "        filePaths.append(dicName)\n",
    "    \n",
    "    return filePaths\n",
    "\n",
    "# partition given data set into training and testing data based on cross validation\n",
    "# INPUT: dataset, number of partition (1 if test on training data)\n",
    "# OUTPUT: list of training dataset, list of testing dataset, the length of list\n",
    "def partition(dataSet, numPartition):\n",
    "    if numPartition == 1 or numPartition >= len(dataSet[1:]):\n",
    "        return [dataSet.copy()], [dataSet.copy()], 1\n",
    "    \n",
    "    data, dataHeader, dataLen = dataSet[1:].copy(), dataSet[0], len(dataSet[1:])\n",
    "    trains, tests, partitionLen, extraInstance = [], [], int(dataLen / numPartition), dataLen % numPartition\n",
    "    \n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    for i in range(0, numPartition):\n",
    "        # fill the extra instances that couldn't be divided equally into partition\n",
    "        if i < extraInstance:\n",
    "            testIndex = list((range(i * (partitionLen + 1), (i + 1) * (partitionLen + 1))))\n",
    "        else:\n",
    "            testIndex = list((range(i * partitionLen + extraInstance, (i + 1) * partitionLen + extraInstance)))\n",
    "            \n",
    "        trainIndex = list(set(range(0, dataLen)).difference(set(testIndex)))\n",
    "        \n",
    "        test, train = [dataHeader], [dataHeader]\n",
    "        \n",
    "        for j in testIndex:\n",
    "            test.append(data[j])\n",
    "            \n",
    "        for k in trainIndex:\n",
    "            train.append(data[k])\n",
    "        \n",
    "        tests.append(test)\n",
    "        trains.append(train)\n",
    "    \n",
    "    return trains, tests, numPartition\n",
    "    \n",
    "#### END DATA PROCESSING ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TRAINING STAGE ####\n",
    "# calculate probabilities (prior, posteriors) from the training data, to build a Naive Bayes (NB) model\n",
    "# missing value handling: not contribute to the counts/probability estimates, exclude a missing value by subtract 1 from everywhere it counts before, not delete the whole row!!!\n",
    "# INPUT: usable format dataSet with proper header as first row\n",
    "# OUTPUT: model, a 2-tuple contains a dictionary of normalised counts (probabilities) representing the class \n",
    "# distribution P(c), and a dictionary (one per class) of dictionaries (one per attribute) of dictionaries (keys are attribute \n",
    "# values, values are counts or probabilities) representing the conditional probabilities P(a|c);\n",
    "def train(dataSet):\n",
    "    # class distribution and conditional probabilities\n",
    "    pc, pac = {}, {}  \n",
    "    dataHeader = dataSet[0][0]\n",
    "\n",
    "    for instance in dataSet[1:]:\n",
    "        # count priors and initialize posteriors\n",
    "        classTag, attributes = instance[1], instance[0]\n",
    "        \n",
    "        if classTag in pc:\n",
    "            pc[classTag] += 1\n",
    "        else:\n",
    "            pc[classTag], pac[classTag] = 1, {}\n",
    "        \n",
    "        # count posteriors, pac represent conditional probabilities\n",
    "        for i in range(0, len(attributes)):\n",
    "            attributeName, attributeValue = dataHeader[i], attributes[i]\n",
    "        \n",
    "            if attributeName in pac[classTag]:\n",
    "                if attributeValue in pac[classTag][attributeName]:\n",
    "                    pac[classTag][attributeName][attributeValue] += 1\n",
    "                else:\n",
    "                    pac[classTag][attributeName][attributeValue] = 1\n",
    "            else:\n",
    "                pac[classTag][attributeName] = {}\n",
    "                pac[classTag][attributeName][attributeValue] = 1\n",
    "                \n",
    "    # normalise counts to get posteriors probabilities of each values of each attributes of each classes\n",
    "    # eliminate missing value\n",
    "    for singleClass in pac.keys():\n",
    "        for attribute in pac[singleClass].keys():\n",
    "            valueKeys = pac[singleClass][attribute].keys()\n",
    "            numInstance = pc[singleClass]\n",
    "            \n",
    "            # if there is missing value, not contribute to the counts/probability estimates, however we still maintain ? in model just for calculating entropy\n",
    "            if '?' in valueKeys:\n",
    "                numInstance = numInstance - pac[singleClass][attribute]['?']\n",
    "            \n",
    "            for valueKey in valueKeys:\n",
    "                # infinity if all are '?'\n",
    "                if numInstance == 0:\n",
    "                    pac[singleClass][attribute][valueKey] = float(\"inf\")\n",
    "                else:\n",
    "                    pac[singleClass][attribute][valueKey] = pac[singleClass][attribute][valueKey] / numInstance\n",
    "    \n",
    "    # normalise counts to get priors probabilities of each classes\n",
    "    for singleClass in pc.keys():\n",
    "        pc[singleClass] = pc[singleClass] / len(dataSet[1:])\n",
    "    \n",
    "    return (pc, pac)\n",
    "    \n",
    "#### END TRAINING STAGE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PREDICTING STAGE ####\n",
    "\n",
    "# predict classes distribution for the test dataset, for each probabilities use log-transformation to prevent underflow, for new values using epsilon smoothing method\n",
    "# INPUT: the Naive Bayes model, usable format dataSet with proper header at first row\n",
    "# OUTPUT: a list of class labels\n",
    "def predict(model, dataSet):\n",
    "    dataHeader, predictClass = dataSet[0][0], []\n",
    "    pc, pac = model\n",
    "\n",
    "    for instance in dataSet[1:]:\n",
    "        maxClass, maxProb = \"\", float('-inf')\n",
    "        attributes = instance[0]\n",
    "        \n",
    "        for singleClass in pac.keys():\n",
    "            # calculate probabilities for each class, replace 0 with epsilon\n",
    "            prob = math.log(pc[singleClass], 2)\n",
    "            \n",
    "            for i in range(0, len(attributes)):\n",
    "                attributeName, attributeValue = dataHeader[i], attributes[i]\n",
    "                \n",
    "                if attributeValue == '?':\n",
    "                    prob += 0\n",
    "                elif attributeValue in pac[singleClass][attributeName].keys():\n",
    "                    prob += math.log(pac[singleClass][attributeName][attributeValue], 2)\n",
    "                else:\n",
    "                    prob += math.log(EPSILON, 2)\n",
    "            \n",
    "            if prob > maxProb:\n",
    "                maxClass, maxProb = singleClass, prob\n",
    "        \n",
    "        predictClass.append(maxClass)\n",
    "        \n",
    "    return predictClass\n",
    "    \n",
    "#### END PREDICTING STAGE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### EVALUATION STAGE ####\n",
    "\n",
    "# output evaluation metric(s) for model and test data, or sufficient information so that they can be easily calculated by hand\n",
    "# INPUT: trained model, usable dataset with header for testing\n",
    "# OUTPUT: void, but prints the necessary evaluation metric information\n",
    "# print example:\n",
    "#   A B\n",
    "# A 2 1\n",
    "# B 0 3\n",
    "# meaning: 2 instances of class A are correctly identified as A, 1 is mistakenly identified as B, all 3 instances of B are correctly identified as B, none of them is mistakenly identified as A\n",
    "def evaluate(model, testSet, num):\n",
    "    # get correct label array\n",
    "    correctLabels, resultLabels = [], []\n",
    "    for instance in testSet[1:]:\n",
    "        correctLabels.append(instance[1])\n",
    "    \n",
    "    resultLabels = predict(model, testSet)\n",
    "    \n",
    "    # get result 2d map, primary key is each primary class, secondary key and value are the number of other class primary class has been identified into\n",
    "    # ex. {'A': {'A': 2, 'B': 1}} means for total 3 test instances of A, 2 of them are correctly identified as A itself, 1 is mistakenly identified as B\n",
    "    metricMap = {}\n",
    "    for i in range(0, len(correctLabels)):\n",
    "        correctLabel, resultLabel = correctLabels[i], resultLabels[i]\n",
    "        \n",
    "        if correctLabel in metricMap.keys():\n",
    "            if resultLabel in metricMap[correctLabel].keys():\n",
    "                metricMap[correctLabel][resultLabel] += 1\n",
    "            else:\n",
    "                metricMap[correctLabel][resultLabel] = 1\n",
    "        else: \n",
    "            metricMap[correctLabel] = {}\n",
    "            metricMap[correctLabel][resultLabel] = 1\n",
    "            \n",
    "    # transform the result map to output matrix format and output results       \n",
    "    print(\"\\nClass Idendification Initial Number Matrix:\")\n",
    "    print(\"\\t\" + \"\".join(key + \"\\t\" for key in metricMap.keys()))\n",
    "    \n",
    "    for priKey in metricMap.keys():\n",
    "        countList = [priKey]\n",
    "        \n",
    "        for secKey in metricMap.keys():\n",
    "            if secKey in metricMap[priKey].keys():\n",
    "                countList.append(metricMap[priKey][secKey])\n",
    "            else:\n",
    "                countList.append(0)\n",
    "                \n",
    "        print(\"\".join(str(count) + \"\\t\" for count in countList))\n",
    "        \n",
    "    # transform the result map to precision, recall, f1-score by assume each class is the interested one separately\n",
    "    print(\"\\nClass Idendification Advanced Data Matrix:\")\n",
    "    print(\"\\t\" + \"\".join(key + \"\\t\" for key in [\"precision\", \"recall\", \"f1-score\"]))\n",
    "    \n",
    "    # get TP, FP, FN for each class and print its precision, recall and f1 score\n",
    "    # calculate and output micro-averaging, macro-averaging, weighted-averaging\n",
    "    macroavg, weightedavg, microavg = [\"macro avg\", 0, 0], [\"weighted avg\", 0, 0], [\"micro avg\"]\n",
    "    tps, fps, fns, precisions, recalls = 0, 0, 0, 0, 0\n",
    "    for priKey in metricMap.keys():\n",
    "        tp, fp, fn, precision, recall, f1, countList = 0, 0, 0, 0, 0, 0, [priKey]\n",
    "        \n",
    "        # get tp, fp, fn for each class\n",
    "        for secKey in metricMap.keys():\n",
    "            if secKey in metricMap[priKey].keys():\n",
    "                if secKey == priKey:\n",
    "                    tp = metricMap[priKey][secKey]\n",
    "                else:\n",
    "                    fn += metricMap[priKey][secKey]\n",
    "            \n",
    "            if priKey in metricMap[secKey].keys() and priKey != secKey:\n",
    "                fp += metricMap[secKey][priKey]\n",
    "        \n",
    "        # calculate precision and recall, f1 for each class     \n",
    "        if (tp + fp) > 0:\n",
    "            precision = tp / (tp + fp)\n",
    "        if (tp + fn) > 0:\n",
    "            recall = tp / (tp + fn)\n",
    "        if (precision + recall) > 0:\n",
    "            f1 = (2 * precision * recall) / (precision + recall)\n",
    "            \n",
    "        countList.extend([precision , recall, f1])\n",
    "        print(\"\".join(str(count) + \"\\t\" for count in countList))\n",
    "        \n",
    "        tps += tp\n",
    "        fps += fp\n",
    "        fns += fn\n",
    "        macroavg[1] += precision / len(metricMap.keys())\n",
    "        macroavg[2] += recall / len(metricMap.keys())\n",
    "        if priKey in model[0].keys():\n",
    "            weightedavg[1] += precision * model[0][priKey]\n",
    "            weightedavg[2] += recall * model[0][priKey]\n",
    "    \n",
    "    if not tps + fps == 0:\n",
    "        precisionu = tps / (tps + fps)\n",
    "    else:\n",
    "        precisionu = 0.0\n",
    "        \n",
    "    if not tps + fns == 0:\n",
    "        recallu = tps / (tps + fns)\n",
    "    else:\n",
    "        recallu = 0.0\n",
    "    \n",
    "    # calculate f1 score for different average\n",
    "    if not macroavg[1] + macroavg[2] == 0:\n",
    "        macroavg.append((2 * macroavg[1] * macroavg[2]) / (macroavg[1] + macroavg[2]))\n",
    "    else:\n",
    "        macroavg.append(0.0)\n",
    "    \n",
    "    if not weightedavg[1] + weightedavg[2] == 0:\n",
    "        weightedavg.append((2 * weightedavg[1] * weightedavg[2]) / (weightedavg[1] + weightedavg[2]))\n",
    "    else: \n",
    "        weightedavg.append(0.0)\n",
    "        \n",
    "    if not precisionu + recallu == 0:\n",
    "        microavg.extend([precisionu, recallu, (2 * precisionu * recallu) / (precisionu + recallu)])    \n",
    "    else:\n",
    "        microavg.extend([0.0, 0.0, 0.0])\n",
    "    \n",
    "    print(\"\".join(str(count) + \"\\t\" for count in macroavg))\n",
    "    print(\"\".join(str(count) + \"\\t\" for count in weightedavg))\n",
    "    print(\"\".join(str(count) + \"\\t\" for count in microavg))\n",
    "\n",
    "    return\n",
    "    \n",
    "#### END EVALUATION ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INFORMATION GAIN ####\n",
    "\n",
    "# calculate the Information Gain (IG) for one (or each) attribute, relative to the class distribution\n",
    "# if there is a missing value, exclude a missing value by subtract 1 from everywhere it counts before, but not delete the whole row!!!\n",
    "# INPUT: A trained model\n",
    "# OUTPUT: a dictionary maps each attribute to its values of Information Gain\n",
    "def info_gain(model):\n",
    "    pc, pac = model\n",
    "    ig, attributes_values, output = 0, {}, {}\n",
    "    \n",
    "    # preparing, get useful data\n",
    "    # get entire attributes and value mapping list\n",
    "    attributes = []\n",
    "    for eachclass in pac:\n",
    "        attributes.extend(pac[eachclass].keys())  \n",
    "    # get unique attributes set\n",
    "    attributes = list(set(attributes))\n",
    "    \n",
    "    for attribute in attributes:\n",
    "        values = []        \n",
    "        for eachclass in pac:\n",
    "            values.extend(pac[eachclass][attribute].keys())\n",
    "        \n",
    "        values = list(set(values))\n",
    "        attributes_values[attribute] = values\n",
    "    \n",
    "    # info_gain calculation\n",
    "    for attribute in attributes_values.keys():\n",
    "        meanInfo, pc_copy, ig = 0, pc.copy(), 0\n",
    "        \n",
    "        # re-calculate the probabilities distribution of each class and entropy of the root when missing value '?' presents\n",
    "        # subtract the ? from everywhere it counts to avoid wrong entropy\n",
    "        if '?' in attributes_values[attribute]:\n",
    "            # get the probability of missing value appeared in all instances\n",
    "            missValueProb, class_prob = 0, {}\n",
    "            for eachclass in pc_copy.keys():\n",
    "                classProb = 0\n",
    "                for value in pac[eachclass][attribute].keys():\n",
    "                    classProb += pac[eachclass][attribute][value]\n",
    "                \n",
    "                class_prob[eachclass] = classProb\n",
    "                missValueProb += pc_copy[eachclass] - pc_copy[eachclass] / classProb\n",
    "                    \n",
    "            # re-calculate the probabilities distribution of each class and entropy of the root\n",
    "            for eachclass in pc_copy.keys():\n",
    "                pc_copy[eachclass] = pc_copy[eachclass] / class_prob[eachclass] / (1 - missValueProb)\n",
    "                \n",
    "                # when the refined probability of a class is 0 (all instances of this class has attribute = '?'), add nothing (0) to ig\n",
    "                if not pc_copy[eachclass] == 0:\n",
    "                    ig -= pc_copy[eachclass] * math.log(pc_copy[eachclass], 2)\n",
    "        \n",
    "        else:\n",
    "            # entropy of the root with no missing value, the entropy before splitting the tree using the attribute’s values\n",
    "            for eachclass in pc.keys():\n",
    "                ig -= pc[eachclass] * math.log(pc[eachclass], 2)\n",
    "        \n",
    "        # the weighted average of the entropy over the children after the split (Mean Information)\n",
    "        # Mean Information (attribute a) = sum_v( P(value v) * H(value v) )\n",
    "        for value in attributes_values[attribute]:\n",
    "            if value == '?':\n",
    "                continue\n",
    "        \n",
    "            prob_av, h_av = 0, 0\n",
    "            \n",
    "            # P(a=v), prob_av\n",
    "            for eachclass in pac.keys():\n",
    "                if attribute in pac[eachclass].keys() and value in pac[eachclass][attribute].keys():\n",
    "                    prob_av += pac[eachclass][attribute][value] * pc_copy[eachclass]\n",
    "        \n",
    "            # H(a=v) =  - sum_c( P(class c | a=v) * log(P(c | a=v)) ), h_av\n",
    "            # P(c | a=v) = (P(a=v | c) * P(c)) / P(a=v), prob_c_av\n",
    "            for eachclass in pac.keys():\n",
    "                if attribute in pac[eachclass].keys() and value in pac[eachclass][attribute].keys():\n",
    "                    prob_c_av = (pac[eachclass][attribute][value] * pc_copy[eachclass]) / prob_av\n",
    "                    h_av -= prob_c_av * math.log(prob_c_av, 2)          \n",
    "                \n",
    "            meanInfo += prob_av * h_av\n",
    "            \n",
    "        output[attribute] = ig - meanInfo\n",
    "\n",
    "    return output\n",
    "    \n",
    "#### END INFORMATION GAIN ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File: hypothyroid.csv\n",
      "\n",
      "No.1 model: \n",
      "\n",
      "Class Idendification Initial Number Matrix:\n",
      "\thypothyroid\tnegative\t\n",
      "hypothyroid\t0\t151\t\n",
      "negative\t0\t3012\t\n",
      "\n",
      "Class Idendification Advanced Data Matrix:\n",
      "\tprecision\trecall\tf1-score\t\n",
      "hypothyroid\t0\t0.0\t0\t\n",
      "negative\t0.9522605121719886\t1.0\t0.9755465587044534\t\n",
      "macro avg\t0.4761302560859943\t0.5\t0.4877732793522267\t\n",
      "weighted avg\t0.9068000830420581\t0.9522605121719886\t0.9289744656395238\t\n",
      "micro avg\t0.9522605121719886\t0.9522605121719886\t0.9522605121719886\t\n",
      "\n",
      "information gain list:  [('TSH', 0.009353710215580346), ('TT4', 0.0057925537058469145), ('T4U', 0.005768288201614458), ('FTI', 0.005744031245602799), ('T3', 0.004075493419623877), ('TBG', 0.002580427555574416), ('query-hypothyroid', 0.0013683791752742147), ('query-on-thyroxine', 0.0012382074503017315), ('surgery', 0.0009985293906336068), ('on-thyroxine', 0.0009139351160850073), ('tumor', 0.0008983004044028076), ('query-hyperthyroid', 0.0005423006444424394), ('sick', 0.0004888757691284829), ('pregnant', 0.000435093846463952), ('sex', 0.00027245774539969014), ('on_antithyroid', 0.00014844815831743796), ('goitre', 7.868469847932547e-05), ('lithium', 4.463778824304043e-05)]\n",
      "\n",
      "File: primary-tumor.csv\n",
      "\n",
      "No.1 model: \n",
      "\n",
      "Class Idendification Initial Number Matrix:\n",
      "\tA\tB\tC\tD\tE\tF\tG\tH\tJ\tK\tL\tM\tN\tO\tP\tQ\tR\tS\tT\tU\tV\t\n",
      "A\t66\t2\t2\t2\t2\t0\t0\t0\t0\t4\t3\t0\t1\t0\t0\t1\t1\t0\t0\t0\t0\t\n",
      "B\t0\t20\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "C\t4\t1\t2\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t\n",
      "D\t2\t0\t0\t5\t0\t0\t0\t0\t0\t0\t0\t0\t4\t0\t0\t1\t0\t1\t0\t0\t1\t\n",
      "E\t4\t0\t0\t0\t11\t0\t0\t1\t0\t5\t3\t0\t3\t0\t0\t1\t8\t0\t0\t0\t3\t\n",
      "F\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "G\t0\t0\t0\t0\t3\t0\t1\t0\t0\t2\t5\t0\t1\t0\t0\t0\t1\t0\t1\t0\t0\t\n",
      "H\t0\t1\t0\t0\t1\t0\t0\t2\t0\t1\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "J\t0\t0\t0\t0\t0\t0\t0\t0\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "K\t3\t0\t0\t0\t2\t0\t0\t0\t0\t13\t5\t0\t0\t0\t0\t0\t5\t0\t0\t0\t0\t\n",
      "L\t0\t0\t0\t0\t1\t0\t0\t0\t0\t2\t12\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t\n",
      "M\t2\t0\t0\t1\t0\t1\t0\t0\t0\t0\t1\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "N\t4\t0\t0\t1\t3\t1\t0\t1\t0\t1\t1\t0\t8\t0\t0\t3\t1\t0\t0\t0\t0\t\n",
      "O\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t\n",
      "P\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t\n",
      "Q\t0\t0\t0\t0\t1\t0\t1\t0\t0\t1\t0\t0\t0\t0\t0\t7\t0\t0\t0\t0\t0\t\n",
      "R\t0\t0\t0\t0\t1\t0\t0\t0\t0\t1\t2\t0\t0\t0\t0\t0\t25\t0\t0\t0\t0\t\n",
      "S\t0\t0\t0\t1\t0\t0\t0\t0\t0\t3\t0\t0\t0\t0\t0\t0\t0\t2\t0\t0\t0\t\n",
      "T\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t2\t0\t0\t\n",
      "U\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t\n",
      "V\t0\t0\t0\t2\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t20\t\n",
      "\n",
      "Class Idendification Advanced Data Matrix:\n",
      "\tprecision\trecall\tf1-score\t\n",
      "A\t0.7764705882352941\t0.7857142857142857\t0.7810650887573966\t\n",
      "B\t0.8333333333333334\t1.0\t0.9090909090909091\t\n",
      "C\t0.5\t0.2222222222222222\t0.30769230769230765\t\n",
      "D\t0.4166666666666667\t0.35714285714285715\t0.3846153846153846\t\n",
      "E\t0.39285714285714285\t0.28205128205128205\t0.32835820895522383\t\n",
      "F\t0.3333333333333333\t1.0\t0.5\t\n",
      "G\t0.5\t0.07142857142857142\t0.125\t\n",
      "H\t0.5\t0.3333333333333333\t0.4\t\n",
      "J\t1.0\t1.0\t1.0\t\n",
      "K\t0.38235294117647056\t0.4642857142857143\t0.41935483870967744\t\n",
      "L\t0.375\t0.75\t0.5\t\n",
      "M\t1.0\t0.2857142857142857\t0.4444444444444445\t\n",
      "N\t0.4444444444444444\t0.3333333333333333\t0.380952380952381\t\n",
      "O\t1.0\t0.5\t0.6666666666666666\t\n",
      "P\t1.0\t1.0\t1.0\t\n",
      "Q\t0.5\t0.7\t0.5833333333333334\t\n",
      "R\t0.5952380952380952\t0.8620689655172413\t0.7042253521126761\t\n",
      "S\t0.6666666666666666\t0.3333333333333333\t0.4444444444444444\t\n",
      "T\t0.6666666666666666\t1.0\t0.8\t\n",
      "U\t1.0\t1.0\t1.0\t\n",
      "V\t0.8333333333333334\t0.8333333333333334\t0.8333333333333334\t\n",
      "macro avg\t0.6531601529500689\t0.6244743579718949\t0.6384952248542257\t\n",
      "weighted avg\t0.6072155705940194\t0.6017699115044246\t0.6044804765500369\t\n",
      "micro avg\t0.6017699115044248\t0.6017699115044248\t0.6017699115044248\t\n",
      "\n",
      "information gain list:  [('degree-of-diffe', 0.4976504889915043), ('histologic-type', 0.4966612887219535), ('sex', 0.32259941861986086), ('neck', 0.2915301360224931), ('axillar', 0.24064185555570194), ('peritoneum', 0.22052193470670556), ('bone', 0.2124618990481668), ('liver', 0.1997614363902529), ('mediastinum', 0.1842576717153852), ('abdominal', 0.17014811083887293), ('age', 0.15474214188706004), ('supraclavicular', 0.12715354518198296), ('lung', 0.10088123982399111), ('pleura', 0.06787277570442418), ('brain', 0.06714460241010611), ('skin', 0.054457867663667336), ('bone-marrow', 0.02036693884804963)]\n",
      "\n",
      "File: hepatitis.csv\n",
      "\n",
      "No.1 model: \n",
      "\n",
      "Class Idendification Initial Number Matrix:\n",
      "\tLIVE\tDIE\t\n",
      "LIVE\t107\t16\t\n",
      "DIE\t9\t23\t\n",
      "\n",
      "Class Idendification Advanced Data Matrix:\n",
      "\tprecision\trecall\tf1-score\t\n",
      "LIVE\t0.9224137931034483\t0.8699186991869918\t0.8953974895397491\t\n",
      "DIE\t0.5897435897435898\t0.71875\t0.6478873239436619\t\n",
      "macro avg\t0.756078691423519\t0.7943343495934959\t0.7747345509935136\t\n",
      "weighted avg\t0.8537334930549614\t0.8387096774193548\t0.8461549020420617\t\n",
      "micro avg\t0.8387096774193549\t0.8387096774193549\t0.8387096774193549\t\n",
      "\n",
      "information gain list:  [('ascites', 0.13261907663068506), ('spiders', 0.1096296962779183), ('histology', 0.08493296456638755), ('fatigue', 0.08483780771287608), ('malaise', 0.08159470186054496), ('varices', 0.0793885051005242), ('spleen-palpable', 0.0366266488362722), ('sex', 0.03660746514280977), ('antivirals', 0.014490701150154384), ('steroid', 0.013190307861426498), ('anorexia', 0.011721480831543674), ('liver-big', 0.004741949313013749), ('liver-firm', 0.0028516574148006457)]\n",
      "\n",
      "File: anneal.csv\n",
      "\n",
      "No.1 model: \n",
      "\n",
      "Class Idendification Initial Number Matrix:\n",
      "\t3\tU\t1\t5\t2\t\n",
      "3\t677\t2\t0\t0\t5\t\n",
      "U\t0\t40\t0\t0\t0\t\n",
      "1\t0\t0\t8\t0\t0\t\n",
      "5\t0\t0\t0\t67\t0\t\n",
      "2\t1\t0\t0\t0\t98\t\n",
      "\n",
      "Class Idendification Advanced Data Matrix:\n",
      "\tprecision\trecall\tf1-score\t\n",
      "3\t0.9985250737463127\t0.9897660818713451\t0.9941262848751836\t\n",
      "U\t0.9523809523809523\t1.0\t0.975609756097561\t\n",
      "1\t1.0\t1.0\t1.0\t\n",
      "5\t1.0\t1.0\t1.0\t\n",
      "2\t0.9514563106796117\t0.98989898989899\t0.9702970297029703\t\n",
      "macro avg\t0.9804724673613754\t0.995933014354067\t0.9881422703430609\t\n",
      "weighted avg\t0.9914037453173692\t0.9910913140311803\t0.9912475050554723\t\n",
      "micro avg\t0.9910913140311804\t0.9910913140311804\t0.9910913140311804\t\n",
      "\n",
      "information gain list:  [('surface-quality', 0.43517783626288564), ('family', 0.40908953764451017), ('steel', 0.3060515354289405), ('formability', 0.29223544065798435), ('hardness', 0.29108220585994726), ('condition', 0.21372288031590858), ('temper_rolling', 0.14711886228095583), ('non-ageing', 0.14107379163812905), ('ferro', 0.13718113252042574), ('strength', 0.12616633610360983), ('chrom', 0.11722522630372056), ('carbon', 0.051344088764403883), ('shape', 0.043239605565149386), ('bf', 0.03935557414283708), ('enamelability', 0.03870173274881061), ('bw-me', 0.037997478813511565), ('bl', 0.036703081364408474), ('oil', 0.03303757117705719), ('surface-finish', 0.032488406491841815), ('phos', 0.029753745208638938), ('cbond', 0.027042353328676993), ('bbvc', 0.0223970898516459), ('bt', 0.021775078259213876), ('bore', 0.01937886432831948), ('lustre', 0.01824168402125048), ('exptl', 0.015604780443500665), ('packing', 0.003958783545891853), ('bc', 0.00043760652021229696), ('product-type', 0.0), ('marvi', 0.0), ('s', 0.0), ('p', 0.0), ('m', 0.0), ('corr', 0.0), ('jurofm', 0.0)]\n",
      "\n",
      "File: test.csv\n",
      "\n",
      "No.1 model: \n",
      "\n",
      "Class Idendification Initial Number Matrix:\n",
      "\tno\tyes\t\n",
      "no\t4\t1\t\n",
      "yes\t0\t9\t\n",
      "\n",
      "Class Idendification Advanced Data Matrix:\n",
      "\tprecision\trecall\tf1-score\t\n",
      "no\t1.0\t0.8\t0.888888888888889\t\n",
      "yes\t0.9\t1.0\t0.9473684210526316\t\n",
      "macro avg\t0.95\t0.9\t0.9243243243243242\t\n",
      "weighted avg\t0.9357142857142857\t0.9285714285714286\t0.9321291735084839\t\n",
      "micro avg\t0.9285714285714286\t0.9285714285714286\t0.9285714285714286\t\n",
      "\n",
      "information gain list:  [('outlook', 0.2467498197744391), ('humidity', 0.15183550136234159), ('windy', 0.04812703040826927), ('temperature', 0.029222565658954536)]\n",
      "\n",
      "File: cmc.csv\n",
      "\n",
      "No.1 model: \n",
      "\n",
      "Class Idendification Initial Number Matrix:\n",
      "\tNo-use\tLong-term\tShort-term\t\n",
      "No-use\t373\t135\t121\t\n",
      "Long-term\t62\t191\t80\t\n",
      "Short-term\t173\t157\t181\t\n",
      "\n",
      "Class Idendification Advanced Data Matrix:\n",
      "\tprecision\trecall\tf1-score\t\n",
      "No-use\t0.6134868421052632\t0.5930047694753577\t0.6030719482619239\t\n",
      "Long-term\t0.39544513457556935\t0.5735735735735735\t0.4681372549019608\t\n",
      "Short-term\t0.4738219895287958\t0.3542074363992172\t0.40537513997760355\t\n",
      "macro avg\t0.4942513220698761\t0.5069285931493828\t0.5005096957108537\t\n",
      "weighted avg\t0.5157430347230753\t0.5057705363204344\t0.5107081073997946\t\n",
      "micro avg\t0.5057705363204344\t0.5057705363204344\t0.5057705363204344\t\n",
      "\n",
      "information gain list:  [('n-child', 0.10173991727554088), ('w-education', 0.07090633894894594), ('h-education', 0.0401385992293839), ('standard-of-living', 0.032511460053806784), ('h-occupation', 0.030474214560266555), ('media-exposure', 0.01578645559562042), ('w-relation', 0.009820501434385065), ('w-work', 0.002582332379721608)]\n",
      "\n",
      "File: car.csv\n",
      "\n",
      "No.1 model: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Idendification Initial Number Matrix:\n",
      "\tunacc\tacc\tvgood\tgood\t\n",
      "unacc\t1161\t47\t0\t2\t\n",
      "acc\t85\t289\t0\t10\t\n",
      "vgood\t0\t26\t39\t0\t\n",
      "good\t0\t46\t2\t21\t\n",
      "\n",
      "Class Idendification Advanced Data Matrix:\n",
      "\tprecision\trecall\tf1-score\t\n",
      "unacc\t0.9317817014446228\t0.959504132231405\t0.9454397394136809\t\n",
      "acc\t0.7083333333333334\t0.7526041666666666\t0.7297979797979798\t\n",
      "vgood\t0.9512195121951219\t0.6\t0.7358490566037735\t\n",
      "good\t0.6363636363636364\t0.30434782608695654\t0.411764705882353\t\n",
      "macro avg\t0.8069245458341786\t0.654114031246257\t0.72252803706509\t\n",
      "weighted avg\t0.8710614687209302\t0.8738425925925926\t0.8724498143028194\t\n",
      "micro avg\t0.8738425925925926\t0.8738425925925926\t0.8738425925925926\t\n",
      "\n",
      "information gain list:  [('safety', 0.26218435655426386), ('persons', 0.2196629633399081), ('buying', 0.09644896916961399), ('maint', 0.07370394692148596), ('lug_boot', 0.030008141247605202), ('doors', 0.004485716626631886)]\n",
      "\n",
      "File: breast-cancer.csv\n",
      "\n",
      "No.1 model: \n",
      "\n",
      "Class Idendification Initial Number Matrix:\n",
      "\trecurrence-events\tno-recurrence-events\t\n",
      "recurrence-events\t47\t38\t\n",
      "no-recurrence-events\t31\t170\t\n",
      "\n",
      "Class Idendification Advanced Data Matrix:\n",
      "\tprecision\trecall\tf1-score\t\n",
      "recurrence-events\t0.6025641025641025\t0.5529411764705883\t0.5766871165644172\t\n",
      "no-recurrence-events\t0.8173076923076923\t0.845771144278607\t0.8312958435207823\t\n",
      "macro avg\t0.7099358974358974\t0.6993561603745977\t0.7046063171806032\t\n",
      "weighted avg\t0.7534852967545276\t0.7587412587412589\t0.756104143820075\t\n",
      "micro avg\t0.7587412587412588\t0.7587412587412588\t0.7587412587412588\t\n",
      "\n",
      "information gain list:  [('deg-malig', 0.07700985251661441), ('inv-nodes', 0.06899508808988597), ('tumor-size', 0.0571711253242968), ('node-caps', 0.05436686791121603), ('irradiat', 0.025819023909140926), ('age', 0.010605956535614025), ('breast-quad', 0.008956416585985671), ('breast', 0.0024889884332655043), ('menopause', 0.0020016149737116518)]\n",
      "\n",
      "File: nursery.csv\n",
      "\n",
      "No.1 model: \n",
      "\n",
      "Class Idendification Initial Number Matrix:\n",
      "\trecommend\tpriority\tnot_recom\tvery_recom\tspec_prior\t\n",
      "recommend\t0\t0\t0\t2\t0\t\n",
      "priority\t0\t3852\t0\t0\t414\t\n",
      "not_recom\t0\t0\t4320\t0\t0\t\n",
      "very_recom\t0\t308\t0\t20\t0\t\n",
      "spec_prior\t0\t532\t0\t0\t3512\t\n",
      "\n",
      "Class Idendification Advanced Data Matrix:\n",
      "\tprecision\trecall\tf1-score\t\n",
      "recommend\t0\t0.0\t0\t\n",
      "priority\t0.8209718670076727\t0.9029535864978903\t0.8600133958472874\t\n",
      "not_recom\t1.0\t1.0\t1.0\t\n",
      "very_recom\t0.9090909090909091\t0.06097560975609756\t0.11428571428571428\t\n",
      "spec_prior\t0.8945491594498217\t0.8684470820969338\t0.8813048933500628\t\n",
      "macro avg\t0.7249223871096807\t0.5664752556701843\t0.6359785413500205\t\n",
      "weighted avg\t0.9057102317632428\t0.9030864197530865\t0.9043964227279211\t\n",
      "micro avg\t0.9030864197530865\t0.9030864197530865\t0.9030864197530865\t\n",
      "\n",
      "information gain list:  [('health', 0.9587749604699762), ('has_nurs', 0.19644928048811572), ('parents', 0.07293460750309944), ('social', 0.022232616894017898), ('housing', 0.01960202502287145), ('children', 0.011886431475775838), ('form', 0.005572591715219621), ('finance', 0.0043331270252002785)]\n",
      "\n",
      "File: mushroom.csv\n",
      "\n",
      "No.1 model: \n",
      "\n",
      "Class Idendification Initial Number Matrix:\n",
      "\tp\te\t\n",
      "p\t3913\t3\t\n",
      "e\t31\t4177\t\n",
      "\n",
      "Class Idendification Advanced Data Matrix:\n",
      "\tprecision\trecall\tf1-score\t\n",
      "p\t0.9921399594320487\t0.9992339121552605\t0.9956743002544529\t\n",
      "e\t0.9992822966507177\t0.9926330798479087\t0.9959465903671912\t\n",
      "macro avg\t0.9957111280413832\t0.9959334960015847\t0.9958222996077457\t\n",
      "weighted avg\t0.9958394861450175\t0.9958148695224026\t0.9958271776815808\t\n",
      "micro avg\t0.9958148695224027\t0.9958148695224027\t0.9958148695224027\t\n",
      "\n",
      "information gain list:  [('odor', 0.9060749773839999), ('spore-print-color', 0.4807049176849155), ('gill-color', 0.41697752341613137), ('ring-type', 0.3180215107935378), ('stalk-surface-above-ring', 0.2847255992184845), ('stalk-surface-below-ring', 0.2718944733927464), ('stalk-color-above-ring', 0.2538451734622398), ('stalk-color-below-ring', 0.24141556652756657), ('gill-size', 0.23015437514804604), ('population', 0.20195801906685262), ('bruises', 0.19237948576121966), ('habitat', 0.1568336046050921), ('gill-spacing', 0.10088318399657026), ('stalk-root', 0.0973385899776934), ('cap-shape', 0.048796701935373), ('ring-number', 0.03845266924309054), ('cap-color', 0.03604928297620413), ('cap-surface', 0.028590232773772817), ('veil-color', 0.02381701612091669), ('gill-attachment', 0.014165027250616413), ('stalk-shape', 0.007516772569664543), ('veil-type', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "#### MAIN FUNCTION ####\n",
    "# main function that initialize and execute this program\n",
    "def main(): \n",
    "    # get all file paths and header mapping for each file\n",
    "    fileNames = getFileNames(FOLDER_PATH)\n",
    "    \n",
    "    for fileName in fileNames:\n",
    "        print(\"\\nFile: \" + fileName)\n",
    "\n",
    "        dataSet = preprocess(fileName)\n",
    "        \n",
    "        trainSets, testSets, num = partition(dataSet, NUM_PARTITION)\n",
    "        \n",
    "        for i in range(0, num):\n",
    "            print(\"\\nNo.\" + str(i + 1) + \" model: \")\n",
    "            model = train(trainSets[i])\n",
    "        \n",
    "            evaluate(model, testSets[i], num)\n",
    "        \n",
    "            iglist = info_gain(model)\n",
    "            print(\"\\ninformation gain list: \", sorted(iglist.items() , reverse=True, key=lambda x: x[1]))\n",
    "    \n",
    "    return\n",
    "    \n",
    "# make the main function work\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "#### END MAIN FUNCTION ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions (you may respond in a cell or cells below):\n",
    "\n",
    "1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "3. Since we have gone to all of the effort of calculating Infomation Gain, we might as well use that as a criterion for building a “Decision Stump” (1-R classifier). How does the effectiveness of this classifier compare to Naive Bayes? Identify one or more cases where the effectiveness is notably different, and explain why.\n",
    "4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "5. Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the Naive Bayes classifier? Explain why, or why not.\n",
    "6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would a imputation strategy have any effect on this?\n",
    "\n",
    "Don't forget that groups of 1 student should respond to question (1), and one other question of your choosing. Groups of 2 students should respond to question (1) and question (2), and two other questions of your choosing. Your responses should be about 150-250 words each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answer for Q1: \n",
    "Theoretically, for an attribute, lower mean information lead to higher IG, means that most instances having same class would be distributed into the same values of this attribute. So, for each value, unlike the data distribution in training data, would contain more homogeneous instances, data become more \"predictable\" after filtering with this attribute. \n",
    "\n",
    "Naive Bayes use prior and posterior probabilities to predict the most likely class, where prior probability could be seen as \"fixed\" term for different attributes since it only describes the class distribution on the whole training set. However, posterior probability (P(a=v|c)) could be vary, for attribute with higher IG, since each value contains more homogeneous instances, P(c|a=v) would be higher for one class and lower for the others. Therefore, according to Bayes theory, posterior and final prediction probability would be higher for one specific class and lower for others, lead to more “confident” prediction to that specific class.\n",
    "\n",
    "Therefore, usually NB classifier will perform better with many high IG attributes shown since they provide \"powerful\" evidence that could “divide” classes, give higher posterior probability to correct class, make prediction less ambiguous and bias. For instance, files anneal.csv, nursery.csv, mushroom.csv, high accuracy (> 90%) has been achieved with many high IG attributes (IG > 0.2), for some others like cmc.csv, breast-cancer.csv, low accuracy occurred (just like accuracy we could obtained from dummy classifier) with no high IG attributes.\n",
    "\n",
    "There are exceptions like primary-tumor.csv and hypothyroid.csv, having high accuracy with no high IG attributes or low accuracy with high IG attributes present. For hypothyroid.csv, it is because classes in it are highly unequally distributed (3012 vs 151), choosing the majority is good enough to achieve high accuracy (95%). Also, most attributes only contain two values and not that informative, it is unlikely possible for them to distinguish minority from such large amount of majority thus result in all low IG. But since choosing majority is good enough, high accuracy is achieved even without informative attributes. For primary-tumor.csv, since IG prefers highly-branching attribute and this file contains so many missing values(based on my algorithm are all eliminated), also some attributes and classes only have few instances, so highly-branched attributes that could separate these small classes will get high IG, since it looks divides classes better. But they are not representative to large classes thus could not help predicting these big classes correctly, which finally lead to a low accuracy with the presence of high IG attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer for Q4: \n",
    "As shown in appendix 1, for most of files like anneal, hepatitis and car, precision, recall and f1 score on different average measurements all went down a little bit (0.1 - 0.6) after implementing 10-fold cross validation. It is actually a more realistic proxy for the accuracy since test on training data usually will cause overfitting problem.  Test on training is useful in some cases as descriptive model when we just want to know how well the model describes the dataset. However generally we want a more generic model that could predict unseen instances. In test on training data, all the test data have been put into NB model and adjust both prior and posterior probabilities, change the prediction probability on desired class (input while training) higher. Since the model so \"perfectly\" that even fits outliers, noise and other variance, it is expected that the model will give higher accuracy output. Cross validation introduces a mechanism that still train and test the whole dataset eventually but not in the same run, it partition data to introduce new instances so that it could better reflect generalize case, so that probabilities inside NB model won't be adjusted by test cases to better predict them, the model we get is more realistic and the result of accuracy is more close to general accuracy. Therefore, cross validation helps reduce the overfitting problem thus results in a slightly decrease on accuracy measurements.\n",
    "\n",
    "However, there two exceptions. First one is files like hypothyroid, nursery and mushroom that accuracy didn't decrease a lot (<0.001). It is because they are either having highly unbalanced class distribution (hypothyroid: 3012 vs151) or having attributes that are so informative (with IG > 0.9). These features remain almost the same in both training and testing data after partition, result in nearly the same probabilities measurement in NB models. The test instances don’t introduce new variance or unusual cases that violates these features (high IG attributes or unequal distribution). Therefore, the accuracy keeps almost the same after using cross validation. The other one is happened on primary-tumor that accuracy decreased a lot after implementing cross validation, one possible reason could be there are too many types of classes and attributes with just small amount of instances and many missing values, so that training data after partition is not \"representative\" enough for the whole data set. Also, the testing data introduces too many new instances with missing values that there are few or none evidences in the training data to classify them correctly, as prediction probabilities are all pretty small (including epsilon into it). Consequently the accuracy degrades a lot after implementing cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix 1 Result Summary for train on test and 10-cross validation: \n",
    "|   File Name   |                           Test on                            |                            Train                             | Result                                                       | 10-cross                                                    | Validation                                                   | Result                                                      |\n",
    "| :-----------: | :----------------------------------------------------------: | :----------------------------------------------------------: | ------------------------------------------------------------ | ----------------------------------------------------------- | ------------------------------------------------------------ | ----------------------------------------------------------- |\n",
    "|  avg types  |                          macro avg                           |                         weighted avg                         | micro avg                                                    | macro avg                                                   | weighted avg                                                 | micro avg                                                   |\n",
    "|  hypothyroid  |    Precision: 0.476130<br />Recall: 0.5<br />F1: 0.487773    | Precision: 0.906800 <br />Recall: 0.952261<br />F1: 0.928974 | Precision: 0.952261 <br />Recall: 0.952261<br />F1: 0.952261 | Precision: 0.476128<br />Recall: 0.5<br />F1: 0.487749      | Precision: 0.906776 <br />Recall: 0.952260<br />F1: 0.928917 | Precision: 0.952255<br />Recall: 0.952255<br />F1: 0.952255 |\n",
    "| primary-tumor | Precision: 0.653160<br />Recall: 0.624474<br />F1: 0.638495  | Precision: 0.607216<br />Recall: 0.601770<br />F1: 0.604480  | Precision: 0.601770<br />Recall: 0.601770<br />F1: 0.601770  | Precision: 0.303401<br />Recall: 0.365846<br />F1: 0.330506 | Precision: 0.384844<br />Recall: 0.452285<br />F1: 0.414059  | Precision: 0.501003<br />Recall: 0.501003<br />F1: 0.501003 |\n",
    "|   hepatitis   | Precision: 0.756079<br />Recall: 0.794334<br />F1: 0.774735  | Precision: 0.853733<br />Recall: 0.838710<br />F1: 0.846155  | Precision: 0.838710<br />Recall: 0.838710<br />F1: 0.838710  | Precision: 0.713810<br />Recall: 0.760638<br />F1: 0.734440 | Precision: 0.828765<br />Recall: 0.823984<br />F1: 0.825496  | Precision: 0.830000<br />Recall: 0.830000<br />F1: 0.830000 |\n",
    "|    anneal     | Precision: 0.980472 <br />Recall: 0.995933<br />F1: 0.988142 | Precision: 0.991404<br />Recall: 0.991091<br />F1: 0.991248  | Precision: 0.991091 <br />Recall: 0.991091<br />F1: 0.991091 | Precision: 0.960640<br />Recall: 0.974651<br />F1: 0.967565 | Precision: 0.987445<br />Recall: 0.986822<br />F1: 0.987129  | Precision: 0.989975<br />Recall: 0.989975<br />F1: 0.989975 |\n",
    "|      cmc      | Precision: 0.494251 <br />Recall: 0.506929<br />F1: 0.500510 | Precision: 0.515743<br />Recall: 0.505771<br />F1: 0.510708  | Precision: 0.505771<br />Recall: 0.505771<br />F1: 0.505771  | Precision: 0.479879<br />Recall: 0.494153<br />F1: 0.486852 | Precision: 0.499999<br />Recall: 0.493428<br />F1: 0.496606  | Precision: 0.490830<br />Recall: 0.490830<br />F1: 0.490830 |\n",
    "|      car      | Precision: 0.806925<br />Recall: 0.654114<br />F1: 0.722528  | Precision: 0.871061<br />Recall: 0.873843<br />F1: 0.872450  | Precision: 0.873843<br />Recall: 0.873843<br />F1: 0.873843  | Precision: 0.782721<br />Recall: 0.609555<br />F1: 0.684041 | Precision: 0.852299<br />Recall: 0.857805<br />F1: 0.855002  | Precision: 0.857652<br />Recall: 0.857652<br />F1: 0.857652 |\n",
    "| breast-cancer | Precision: 0.709936<br />Recall: 0.699356<br />F1: 0.704606  | Precision: 0.753485<br />Recall:  0.758741<br />F1: 0.756104 | Precision: 0.758741<br />Recall: 0.758741<br />F1: 0.758741  | Precision: 0.647610<br />Recall: 0.636138<br />F1: 0.641159 | Precision: 0.697813<br />Recall: 0.716202<br />F1: 0.706702  | Precision: 0.713424<br />Recall: 0.713424<br />F1: 0.713424 |\n",
    "|    nursery    | Precision: 0.724922 <br />Recall: 0.566475<br />F1: 0.635979 | Precision: 0.905710<br />Recall: 0.903086<br />F1: 0.904396  | Precision: 0.903086<br />Recall: 0.903086<br />F1: 0.903086  | Precision: 0.860415<br />Recall: 0.695194<br />F1: 0.767559 | Precision: 0.902317<br />Recall: 0.902713<br />F1: 0.902494  | Precision: 0.902701<br />Recall: 0.902701<br />F1: 0.902701 |\n",
    "|   mushroom    | Precision: 0.995711<br />Recall: 0.995933<br />F1: 0.995822  | Precision: 0.995839 <br />Recall: 0.995815<br />F1: 0.995827 | Precision: 0.995815<br />Recall: 0.995815<br />F1: 0.995815  | Precision: 0.995393<br />Recall: 0.995757<br />F1: 0.995575 | Precision: 0.995523<br />Recall: 0.995638<br />F1: 0.995580  | Precision: 0.995568<br />Recall: 0.995568<br />F1: 0.995568 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
